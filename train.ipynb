{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c7ee6d87614357b26c8940743bbed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 60\u001b[0m\n\u001b[0;32m     55\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m hf_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_and_align_labels, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 60\u001b[0m train_data, val_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(train_data)\n\u001b[0;32m     62\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(val_data)\n",
      "File \u001b[1;32mc:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2805\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m-> 2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2807\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[0;32m   2808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2809\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2807\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m   2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2806\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2807\u001b[0m         (\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2808\u001b[0m     )\n\u001b[0;32m   2809\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:267\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _polars_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32mc:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:33\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     32\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "File \u001b[1;32mc:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2782\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2766\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2764\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2765\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2766\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2767\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2768\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2769\u001b[0m )\n\u001b[0;32m   2770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:612\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[1;32m--> 612\u001b[0m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\KIIT0001\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:562\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[1;34m(key, size)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Iterable):\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 562\u001b[0m         _check_valid_index_key(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m), size\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m    563\u001b[0m         _check_valid_index_key(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(key)), size\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'ellipsis'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# --- Step 1: Load and Preprocess the Dataset ---\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"TASTEset.csv\")\n",
    "\n",
    "# Parse the entities column (convert JSON-like strings to Python dictionaries)\n",
    "def parse_entities(row):\n",
    "    try:\n",
    "        return ast.literal_eval(row[\"ingredients_entities\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing row: {row['ingredients_entities']}\")\n",
    "        return []\n",
    "\n",
    "dataset[\"entities\"] = dataset.apply(parse_entities, axis=1)\n",
    "\n",
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "# --- Step 2: Tokenize and Align Labels ---\n",
    "# Initialize tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define label mappings\n",
    "label2id = {\"O\": 0, \"B-QUANTITY\": 1, \"I-QUANTITY\": 2, \"B-UNIT\": 3, \"I-UNIT\": 4, \"B-FOOD\": 5, \"I-FOOD\": 6}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"ingredients\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    labels = []\n",
    "    \n",
    "    for i, entity_list in enumerate(examples[\"entities\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignore special tokens\n",
    "            elif word_idx < len(entity_list):  # Ensure index is within bounds\n",
    "                label_ids.append(label2id.get(entity_list[word_idx][\"type\"], 0))  # Default to \"O\"\n",
    "            else:\n",
    "                label_ids.append(0)  # Default to \"O\" for out-of-bound indices\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# --- Step 3: Split Dataset into Train and Validation ---\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "# --- Step 4: Fine-Tune DistilBERT ---\n",
    "# Load DistilBERT with a classification head\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label2id))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"./fine_tuned_distilbert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_distilbert\")\n",
    "\n",
    "# --- Step 5: Test the Model ---\n",
    "def extract_ingredients(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    entities = [id2label[pred.item()] for pred in predictions[0]]\n",
    "    \n",
    "    return list(zip(tokens, entities))\n",
    "\n",
    "recipe_text = \"Mix 2 cups of flour with 1 tsp salt\"\n",
    "print(extract_ingredients(recipe_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
